{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWbfqohI+orzvmzMtATEap",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EldorAxmedov/EldorAxmedov-PDO/blob/master/TF_IDF_exaple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Извлечение ключевых слов и терминов**"
      ],
      "metadata": {
        "id": "-m-WkkiW_s3j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcNZ07EN66xu",
        "outputId": "1121f360-4b72-4434-cfb8-c4779b779568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ключевые слова: ['nlp' 'аспект' 'интересная' 'ключевой' 'машинного' 'машинное' 'машинным'\n",
            " 'область' 'обучение' 'обучением' 'обучения' 'связана' 'также' 'учителем'\n",
            " 'это']\n",
            "Значения TF-IDF: [0.         0.         0.49047908 0.         0.         0.49047908\n",
            " 0.         0.37302199 0.37302199 0.         0.         0.\n",
            " 0.         0.         0.49047908]\n",
            "Press Enter to exit...\n",
            "Ключевые слова: ['это', 'машинное', 'интересная', 'обучение', 'область', 'учителем', 'также', 'связана', 'обучения', 'обучением', 'машинным', 'машинного', 'ключевой', 'аспект', 'nlp']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Пример текстовых данных\n",
        "documents = [\n",
        "    \"Машинное обучение - это интересная область.\",\n",
        "    \"Обучение с учителем - ключевой аспект машинного обучения.\",\n",
        "    \"Область NLP также связана с машинным обучением.\"\n",
        "]\n",
        "\n",
        "# Создание объекта TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Применение TF-IDF к текстовым данным\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Получение списка ключевых слов и их значения TF-IDF для первого документа\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "\n",
        "print(\"Ключевые слова:\", feature_names)\n",
        "print(\"Значения TF-IDF:\", tfidf_scores)\n",
        "\n",
        "# Stop program\n",
        "input(\"Press Enter to exit...\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Сортировка слов по значениям TF-IDF\n",
        "sorted_keywords = [word for _, word in sorted(zip(tfidf_scores, feature_names), reverse=True)]\n",
        "\n",
        "print(\"Ключевые слова:\", sorted_keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Обучение модели классификации на основе TF-IDF**"
      ],
      "metadata": {
        "id": "pmtfnPxL_n_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Пример данных для классификации\n",
        "categories = [\"технологии\", \"спорт\", \"политика\"]\n",
        "labels = [0, 1, 2]\n",
        "data = [\"Машинное обучение - это интересная область.\",\n",
        "        \"Обучение с учителем - ключевой аспект машинного обучения.\",\n",
        "        \"Область NLP также связана с машинным обучением.\",\n",
        "        ]\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Преобразование текстовых данных в матрицу TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Обучение модели классификации\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Предсказание категорий для тестовых данных\n",
        "y_pred = classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Оценка точности модели\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Точность модели: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obxQXgIH8Df_",
        "outputId": "7cc0ffa1-5ae5-4d60-92d2-84fb0846960e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность модели: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Применение N-грамм вместе с TF-IDF**"
      ],
      "metadata": {
        "id": "6QTqkHXT_iti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Пример текстовых данных\n",
        "documents = [\n",
        "    \"Искусственный интеллект становится все более распространенным.\",\n",
        "    \"Интеллектуальные системы обладают большим потенциалом.\"\n",
        "]\n",
        "\n",
        "# Создание объекта TfidfVectorizer с использованием биграмм\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "\n",
        "# Применение TF-IDF к текстовым данным\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Получение списка фичей и их значения TF-IDF для первого документа\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_scores = tfidf_matrix.toarray()[0]\n",
        "\n",
        "# Сортировка фичей по значениям TF-IDF\n",
        "sorted_features = [feature for _, feature in sorted(zip(tfidf_scores, feature_names), reverse=True)]\n",
        "\n",
        "print(\"Важные фичи:\", sorted_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGyriN1m-gqg",
        "outputId": "1ca6de54-1a3a-4a59-8232-eca87e216ebb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Важные фичи: ['становится все', 'становится', 'распространенным', 'искусственный интеллект', 'искусственный', 'интеллект становится', 'интеллект', 'все более', 'все', 'более распространенным', 'более', 'системы обладают', 'системы', 'потенциалом', 'обладают большим', 'обладают', 'интеллектуальные системы', 'интеллектуальные', 'большим потенциалом', 'большим']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Удаление стоп-слов и специальных символов**"
      ],
      "metadata": {
        "id": "SHV63nMi_fc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Загрузка стоп-слов и пунктуации\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text.lower())  # Привести к нижнему регистру и токенизировать\n",
        "    filtered_words = [word for word in words if word not in stop_words and word not in punctuation]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "# Пример предобработки текстовых данных\n",
        "text = \"Машинное обучение - это интересная область, изучаемая многими.\"\n",
        "preprocessed_text = preprocess_text(text)\n",
        "print(\"Предобработанный текст:\", preprocessed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0-vuVO5_UMY",
        "outputId": "77bc8698-5fed-4c01-f24d-6d4598e6b411"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предобработанный текст: машинное обучение это интересная область изучаемая многими\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Приведение к нижнему регистру и лемматизация**"
      ],
      "metadata": {
        "id": "lSyzMXVuAAJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Загрузка языковой модели spaCy\n",
        "nlp = spacy.load('uz_core_news_lg')\n",
        "\n",
        "def preprocess_and_lemmatize(text):\n",
        "    doc = nlp(text.lower())  # Привести к нижнему регистру и лемматизировать\n",
        "    lemmatized_words = [token.lemma_ for token in doc if token.text not in punctuation and token.text not in stop_words]\n",
        "    return \" \".join(lemmatized_words)\n",
        "\n",
        "# Пример предобработки и лемматизации текста\n",
        "text = \"Машинное обучение - это интересная область, изучаемая многими.\"\n",
        "preprocessed_lemmatized_text = preprocess_and_lemmatize(text)\n",
        "print(\"Предобработанный и лемматизированный текст:\", preprocessed_lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "gFTIIsqJ_9Qe",
        "outputId": "2e1cb86b-a372-4da4-9ab0-8fb8dd2bbb2e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'uz_core_news_lg'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e46b53cbb1ce>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Загрузка языковой модели spaCy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uz_core_news_lg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_and_lemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'uz_core_news_lg'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ]
    }
  ]
}